ğŸ¤– Personal AI Chatbot (RAG-Based Document Assistant)

Your Personal AI Chatbot is a Retrieval-Augmented Generation (RAG) system that can read your personal documents (PDFs, DOCXs, etc.) and answer questions about them intelligently using OpenAIâ€™s GPT models.

Built with LangChain, Docling, ChromaDB, and Streamlit, it gives you your own private AI assistant â€” one that knows your data.

ğŸ§  Key Features

ğŸ—‚ï¸ Reads and understands PDFs/DOCX using Docling

ğŸ” Retrieval-Augmented Generation (RAG) pipeline for precise answers

ğŸ§­ Semantic Search using ChromaDB

ğŸ’¬ Chat interface built with Streamlit

ğŸ” Private and local â€” your data never leaves your system

âš™ï¸ Embeddings powered by OpenAI (text-embedding-3-small)

ğŸ§© Modular design (Docling â†’ Embeddings â†’ Chroma â†’ GPT â†’ Streamlit)

ğŸ—ï¸ Architecture Overview
graph TD

    A[ğŸ“ Documents (PDF/DOCX)] --> B[DoclingLoader
    B --> C[LangChain Text Splitter]
    C --> D[OpenAI Embeddings]
    D --> E[Chroma Vector DB]
    F[User Question] --> G[OpenAI Embeddings (query)]
    G --> H[Retriever from Chroma]
    H --> I[Relevant Chunks]
    I --> J[Prompt Template + GPT-3.5-turbo]
    J --> K[ğŸ§  Final Answer Shown in Streamlit Chat]

âš™ï¸ Tech Stack
| Component        | Purpose                                      |
| ---------------- | -------------------------------------------- |
| **Python 3.10+** | Core programming language                    |
| **Streamlit**    | Chat UI                                      |
| **LangChain**    | Framework to chain LLM + retrieval           |
| **OpenAI API**   | LLM (GPT) + embeddings                       |
| **Docling**      | Extracts text from PDFs/DOCXs                |
| **ChromaDB**     | Local vector database for document retrieval |
| **dotenv**       | Securely loads API keys                      |

ğŸ“ Folder Structure
personal-ai-chatbot/

â”‚

â”œâ”€â”€ data/

â”‚   â””â”€â”€ docs/                â† place your PDFs/DOCXs here

â”‚

â”œâ”€â”€ chroma_db_openai/        â† auto-generated local vector DB (after running ingest.py)

â”‚

â”œâ”€â”€ .env                     â† stores your OpenAI API key (DO NOT COMMIT)

â”œâ”€â”€ requirements.txt          â† dependencies

â”œâ”€â”€ docling_loader.py         â† handles text extraction

â”œâ”€â”€ ingest.py                 â† builds embeddings + vector DB

â”œâ”€â”€ app.py                    â† Streamlit chat interface

â””â”€â”€ README.md


ğŸ”‘ Environment Setup
1ï¸âƒ£ Clone the repository
git clone https://github.com/yourusername/personal-ai-chatbot.git
cd personal-ai-chatbot

2ï¸âƒ£ Create a virtual environment
python3 -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Window

3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

4ï¸âƒ£ Add your OpenAI API key
Create a file named .env in the root directory:
OPENAI_API_KEY=sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx

5ï¸âƒ£ Add your documents
Place your .pdf or .docx files in:
data/docs/

6ï¸âƒ£ Build your local vector database
Run the ingestion script (this reads, chunks, embeds, and stores your docs):
python3 ingest.py

If successful, youâ€™ll see a new folder chroma_db_openai/.

7ï¸âƒ£ Launch the chatbot
python3 -m streamlit run app.py
Then open the local URL shown in your terminal (usually http://localhost:8501).

ğŸ’¬ How It Works

Document Processing
docling_loader.py uses Docling to extract clean text from your PDFs/DOCXs.

Chunking & Embeddings
ingest.py splits long text into overlapping chunks.
Each chunk is embedded (converted to a vector) using OpenAIâ€™s embedding model.

Vector Storage
The embeddings are stored locally in ChromaDB.

Retrieval + Generation
When you ask a question in the Streamlit UI, LangChain retrieves the most relevant chunks from ChromaDB.
GPT-3.5 reads the context and generates a grounded answer.

Display + Sources
The chatbot responds with an answer and shows which files it used as sources.
